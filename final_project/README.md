# ğŸš€ Final Project: High-Performance GEMM

> ä»é›¶å®ç°é«˜æ€§èƒ½çŸ©é˜µä¹˜æ³•ï¼Œæ€§èƒ½è¾¾åˆ° cuBLAS çš„ 80%+

**è¿™æ˜¯ä¸€ä¸ªå¯ä»¥å†™è¿›ç®€å†çš„ç¡¬æ ¸ CUDA é¡¹ç›®ï¼**

---

## ğŸ“– é¡¹ç›®ç®€ä»‹

GEMM (General Matrix Multiply) æ˜¯æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒç®—å­ã€‚æœ¬é¡¹ç›®ä» Naive å®ç°å¼€å§‹ï¼Œé€æ­¥ä¼˜åŒ–åˆ°æ¥è¿‘ cuBLAS çš„æ€§èƒ½ã€‚

---

## ğŸ¯ é¡¹ç›®ç›®æ ‡

| ç‰ˆæœ¬ | ä¼˜åŒ–æŠ€æœ¯ | é¢„æœŸæ€§èƒ½ |
|------|---------|---------|
| V1 Naive | åŸºç¡€å®ç° | ~100 GFLOPS |
| V2 Tiled | Shared Memory | ~500 GFLOPS |
| V3 Coalescing | å†…å­˜åˆå¹¶ä¼˜åŒ– | ~1000 GFLOPS |
| V4 Vectorized | float4 å‘é‡åŒ– | ~2000 GFLOPS |
| V5 Register Tiling | å¯„å­˜å™¨åˆ†å— | ~4000 GFLOPS |
| V6 Double Buffering | éšè—å»¶è¿Ÿ | ~6000 GFLOPS |
| cuBLAS | NVIDIA å®˜æ–¹ | ~8000 GFLOPS |

**ç›®æ ‡ï¼šè¾¾åˆ° cuBLAS 80% çš„æ€§èƒ½ï¼**

---

## ğŸ“ æ–‡ä»¶ç»“æ„

```
final_project/
â”œâ”€â”€ README.md
â”œâ”€â”€ Makefile
â”œâ”€â”€ include/
â”‚   â””â”€â”€ gemm.h              # å¤´æ–‡ä»¶
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ v1_naive.cu         # V1: Naive å®ç°
â”‚   â”œâ”€â”€ v2_tiled.cu         # V2: Shared Memory Tiling
â”‚   â”œâ”€â”€ v3_coalescing.cu    # V3: å†…å­˜åˆå¹¶ä¼˜åŒ–
â”‚   â”œâ”€â”€ v4_vectorized.cu    # V4: float4 å‘é‡åŒ–
â”‚   â”œâ”€â”€ v5_register.cu      # V5: å¯„å­˜å™¨åˆ†å—
â”‚   â”œâ”€â”€ v6_double_buffer.cu # V6: Double Buffering
â”‚   â””â”€â”€ benchmark.cu        # æ€§èƒ½æµ‹è¯•
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_correctness.cu # æ­£ç¡®æ€§æµ‹è¯•
â””â”€â”€ docs/
    â”œâ”€â”€ optimization_notes.md  # ä¼˜åŒ–ç¬”è®°
    â””â”€â”€ performance_report.md  # æ€§èƒ½æŠ¥å‘Š
```

---

## ğŸ”§ ä¼˜åŒ–æŠ€æœ¯è¯¦è§£

### V1: Naive Implementation
- æ¯ä¸ªçº¿ç¨‹è®¡ç®— C çš„ä¸€ä¸ªå…ƒç´ 
- é—®é¢˜ï¼šå¤§é‡é‡å¤è¯»å– Global Memory

### V2: Shared Memory Tiling
- å°†çŸ©é˜µåˆ†å—åŠ è½½åˆ° Shared Memory
- å‡å°‘ Global Memory è®¿é—®

### V3: Memory Coalescing
- ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
- ç¡®ä¿ Warp å†…çº¿ç¨‹è®¿é—®è¿ç»­åœ°å€

### V4: Vectorized Load/Store
- ä½¿ç”¨ float4 ä¸€æ¬¡è¯»å– 4 ä¸ªå…ƒç´ 
- æé«˜å†…å­˜å¸¦å®½åˆ©ç”¨ç‡

### V5: Register Tiling
- æ¯ä¸ªçº¿ç¨‹è®¡ç®—å¤šä¸ªè¾“å‡ºå…ƒç´ 
- å¢åŠ æ•°æ®å¤ç”¨ï¼Œå‡å°‘ Shared Memory å‹åŠ›

### V6: Double Buffering
- è®¡ç®—å’Œæ•°æ®åŠ è½½é‡å 
- éšè—å†…å­˜å»¶è¿Ÿ

---

## ğŸ“Š æ€§èƒ½æµ‹è¯•

```bash
# ç¼–è¯‘
make

# æŒ‡å®šæ¶æ„ç¼–è¯‘ï¼ˆRTX 5070 Ti å»ºè®®ï¼‰
make CUDA_ARCH=89

# è¿è¡Œ benchmark
./benchmark

# è¾“å‡ºç¤ºä¾‹
Matrix Size: 4096 x 4096
V1 Naive:         120.5 GFLOPS
V2 Tiled:         523.7 GFLOPS (4.3x vs V1)
V3 Coalescing:   1024.3 GFLOPS (8.5x vs V1)
V4 Vectorized:   2156.8 GFLOPS (17.9x vs V1)
V5 Register:     4312.5 GFLOPS (35.8x vs V1)
V6 DoubleBuffer: 6245.2 GFLOPS (51.8x vs V1)
cuBLAS:          7823.4 GFLOPS
Achieved: 79.8% of cuBLAS
```

---

## ğŸ“ ç®€å†å†™æ³•

```
é¡¹ç›®ï¼šHigh-Performance GEMM on CUDA

â€¢ ä»é›¶å®ç° GPU çŸ©é˜µä¹˜æ³•ï¼Œé€šè¿‡ 6 ä¸ªç‰ˆæœ¬è¿­ä»£ä¼˜åŒ–ï¼Œæœ€ç»ˆæ€§èƒ½è¾¾åˆ° cuBLAS çš„ 80%
â€¢ åº”ç”¨ Shared Memory Tilingã€Memory Coalescingã€å‘é‡åŒ–è®¿å­˜ã€å¯„å­˜å™¨åˆ†å—ç­‰ä¼˜åŒ–æŠ€æœ¯
â€¢ ä½¿ç”¨ Nsight Compute è¿›è¡Œæ€§èƒ½åˆ†æï¼Œè¯†åˆ«å¹¶è§£å†³ Memory Bound ç“¶é¢ˆ
â€¢ åœ¨ RTX 5070 Ti ä¸Šå®ç° 6000+ GFLOPSï¼Œç›¸æ¯” Naive ç‰ˆæœ¬æå‡ 50 å€

æŠ€æœ¯æ ˆï¼šCUDA, C++, Nsight Compute
```

---

## ğŸ¯ å­¦ä¹ è·¯çº¿

### Week 1: V1 + V2
- [ ] å®ç° Naive ç‰ˆæœ¬
- [ ] å®ç° Tiled ç‰ˆæœ¬
- [ ] å¯¹æ¯”æ€§èƒ½å·®å¼‚

### Week 2: V3 + V4
- [ ] ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
- [ ] å®ç°å‘é‡åŒ–è¯»å†™
- [ ] ä½¿ç”¨ Nsight åˆ†æ

### Week 3: V5 + V6
- [ ] å®ç°å¯„å­˜å™¨åˆ†å—
- [ ] å®ç° Double Buffering
- [ ] æœ€ç»ˆæ€§èƒ½è°ƒä¼˜

### Week 4: æ–‡æ¡£ + æ€»ç»“
- [ ] æ’°å†™ä¼˜åŒ–ç¬”è®°
- [ ] ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
- [ ] æ•´ç†ä»£ç ï¼Œå‡†å¤‡å¼€æº

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [CUTLASS](https://github.com/NVIDIA/cutlass) - NVIDIA å®˜æ–¹é«˜æ€§èƒ½ GEMM åº“
- [How to Optimize GEMM](https://github.com/flame/how-to-optimize-gemm)
- [CUDA C++ Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)

---

**è¿™ä¸ªé¡¹ç›®å®Œæˆåï¼Œä½ å°±æ˜¯ CUDA é«˜æ‰‹äº†ï¼** ğŸ’ª
